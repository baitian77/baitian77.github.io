<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小骑士</title>
  <subtitle>subtitle</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-05-10T09:39:22.961Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>knightyang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ElasticSearch调研总结</title>
    <link href="http://yoursite.com/2017/05/10/ElasticSearch%E8%B0%83%E7%A0%94%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2017/05/10/ElasticSearch调研总结/</id>
    <published>2017-05-10T09:37:53.142Z</published>
    <updated>2017-05-10T09:39:22.961Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ElasticSearch调研总结"><a href="#ElasticSearch调研总结" class="headerlink" title="ElasticSearch调研总结"></a>ElasticSearch调研总结</h3><h4 id="ElasticSearch简介"><a href="#ElasticSearch简介" class="headerlink" title="ElasticSearch简介"></a>ElasticSearch简介</h4><p>​    Elasticsearch 是一个分布式可扩展的<strong>实时</strong>搜索和分析引擎,一个建立在全文搜索引擎 Apache Lucene(TM) 基础上的搜索引擎。ElasticSearch可以做的事儿:</p>
<ul>
<li>分布式的实时文件存储，并为每一个字段建立索引，默认存储在本地磁盘</li>
<li>实时分析的分布式搜索引擎</li>
<li>集群规模可以动态扩展，具备分布式的基本要求：动态扩展、容错、负载均衡</li>
</ul>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li><p>Cluster (集群)</p>
</li>
<li><p>Node (节点)</p>
<blockquote>
<p>每个节点上运行一个ElasticSearch实例，节点启动后会自动广播查询，配置信息中cluster.name值相同的节点会构造成一个集群</p>
</blockquote>
</li>
<li><p>Shard (分片)</p>
<blockquote>
<p>类似HDFS中的block，会将大容量的数据切分成多个Shard，不同的Shard可以分散存储在集群上不同的节点</p>
</blockquote>
</li>
<li><p>Replia (副本)</p>
<blockquote>
<p>一个Replia是一个Shard的精准复制，每个Shard可含有0个或多个Replia。用于容错、并发查询</p>
</blockquote>
</li>
<li><p>面向文档</p>
<p>​    ElasticSearch是面向文档存储的，基本的存储单位就是文档，一条记录就是一个文档，文档格式统一为JSON格式，例如：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"name"</span> :     <span class="string">"John"</span>,</div><div class="line">    <span class="attr">"sex"</span> :      <span class="string">"Male"</span>,</div><div class="line">    <span class="attr">"age"</span> :      <span class="number">25</span>,</div><div class="line">    <span class="attr">"birthDate"</span>: <span class="string">"1990/05/01"</span>,</div><div class="line">    <span class="attr">"about"</span> :    <span class="string">"I love to go rock climbing"</span>,</div><div class="line">    <span class="attr">"interests"</span>: [ <span class="string">"sports"</span>, <span class="string">"music"</span> ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>index (索引)、type (类型)、id </p>
<p>ES中通过index/type/id 来唯一标示一个文档。与Mysql概念对比：</p>
<p>| Mysql         | ES                  |<br>| ————- | ——————- |<br>| database(数据库) | index(索引)           |<br>| table(表)      | type(类型)            |<br>| row(行)        | 文档                  |<br>| column(列)     | field               |<br>| schema        | mapping             |<br>| index         | everything is index |<br>| sql           | Query DSL           |</p>
</li>
<li><p>Query DSL</p>
<p>Mysql 中采用SQL语句进行查询，ES中统一采用Query DSL查询，详情可参考：<a href="http://www.voidcn.com/blog/bigbigtreewhu/article/p-6323823.html" target="_blank" rel="external">Elasticsearch——Query DSL</a>。<code>select * from bank;</code> 示例：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">curl -XPOST 'localhost:9200/bank/_search?pretty' -d '</div><div class="line">&#123;</div><div class="line">  "query": &#123; "match_all": &#123;&#125; &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h4><ul>
<li><p>版本选择与下载、解压，<a href="[http://www.elasticsearch.org/download/](http://www.elasticsearch.org/download/">下载地址</a> )</p>
<ul>
<li><strong>Note:  调研版本选择的是2.4.1，而非最新版本5.*</strong>(版本号2.4之后就直接是5.* )，原因：<ul>
<li>5.<em> 要求JAVA8，5.</em> 官方插件也要求JAVA8</li>
<li>很多常用的第三方插件（bigdesk、head、sql、jdbc-sql）还未支持5.*</li>
<li>2.4 与 5.* 性能实测差异不大，详见<a href="http://www.ctolib.com/topics/79270.html" target="_blank" rel="external">ElasticSearch 5.0 测评以及使用体验</a></li>
</ul>
</li>
</ul>
</li>
<li><p>配置所有节点，将$ES_HOME/elasticsearch.yml中cluster.name 修改成相同值</p>
</li>
<li><p>$ES_HOME/bin/elasticsearch</p>
<blockquote>
<p>所有节点上执行执行，就启动了ES。会自动广播组成集群。</p>
</blockquote>
<p>JVM默认参数：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">-Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true</div></pre></td></tr></table></figure>
</li>
<li><p>集群访问地址：host:9200</p>
<blockquote>
<p><a href="http://200.200.200.64:9200/" target="_blank" rel="external">http://200.200.200.64:9200/</a></p>
</blockquote>
</li>
<li><p>插件安装</p>
<ul>
<li><p>Note：ES安装包只提供了最基本的功能：本地分布式存储数据、创建索引、提供查询服务等。其余一些额外功能则需要第三方插件支持，如：集群状态监控、SQL转DSL Query、集群性能监控、Mysql数据导入ES等。详见：<a href="http://chenghuiz.iteye.com/blog/2310377" target="_blank" rel="external">elasticsearch以及其常用插件安装</a></p>
</li>
<li><p>访问(host:9200/_plugin/plugin<em>name)</em>，例如:</p>
<blockquote>
<p><a href="http://200.200.200.64:9200/_plugin/head/" target="_blank" rel="external">http://200.200.200.64:9200/_plugin/head/</a></p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h4 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h4><ul>
<li><p>将Mysql单表中的数据导入ES</p>
<ul>
<li><p>Mysql单表数据量：1亿行，16GB；导入ES后，占用内存：32GB</p>
<ul>
<li><strong>Note：第一次导入完成后，发现数据丢失，即从Mysql导入一亿行数据后，ES中count(*)仅有7600万行，后续会有原因分析</strong></li>
</ul>
</li>
<li><p>导入方法：</p>
<ul>
<li><p>安装第三方插件：elasticsearch-jdbc，按照git上的操作指引进行数据导入，git地址：<a href="https://github.com/jprante/elasticsearch-jdbc" target="_blank" rel="external">elasticsearch-jdbc</a>，导入脚本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span>!/bin/sh</div><div class="line">export JAVA_HOME=/home/knightyang/jdk1.8.0_131</div><div class="line">export PATH=$&#123;JAVA_HOME&#125;/bin/:$PATH;</div><div class="line">JDBC_IMPORTER_HOME=/home/knightyang/elasticsearch-jdbc-2.3.4.0</div><div class="line">bin=$JDBC_IMPORTER_HOME/bin</div><div class="line">lib=$JDBC_IMPORTER_HOME/lib</div><div class="line">echo '&#123;</div><div class="line">"type" : "jdbc",</div><div class="line">"jdbc": &#123;</div><div class="line">"elasticsearch.autodiscover":true,</div><div class="line">"url":"jdbc:mysql://host:3306/test",  </div><div class="line">"user":"test",  </div><div class="line">"password":"test",  </div><div class="line">"sql":"select *,id  as _id from user_info_yqj ",</div><div class="line">"elasticsearch" : &#123;</div><div class="line">  "host" : "host",</div><div class="line">  "port" : 9300</div><div class="line">&#125;,</div><div class="line">"index" : "yqj_info_index_more",  </div><div class="line">"type" : "yqj_info_type_more"  </div><div class="line">&#125;</div><div class="line">&#125;'| java \</div><div class="line">  -cp "$&#123;lib&#125;/*" \</div><div class="line">  -Dlog4j.configurationFile=$&#123;bin&#125;/log4j2.xml \</div><div class="line">  org.xbib.tools.Runner \</div><div class="line">  org.xbib.tools.JDBCImporter</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>安装bigdesk插件，监控集群资源状态（Memory、CPU、GC、Thread Pool等）</p>
<blockquote>
<p>用于查看执行SQL时的资源消耗</p>
</blockquote>
</li>
<li><p>安装sql插件，可方便的将sql转换成DSL Query</p>
<blockquote>
<p>SQL:  select count(*) from yqj_info_index_more</p>
</blockquote>
</li>
<li><p>安装head插件，可以方便的提交DSL Query</p>
</li>
<li><p>count(*) 统计一亿行数据耗时对比（清理查询缓存后多次执行，取平均值）</p>
<p>| Mysql | ES    |<br>| —– | —– |<br>| 51秒   | 900毫秒 |</p>
<blockquote>
<p>Note: Mysql 执行时间波动较大，从31秒 ~ 80秒，预计于机器负载有关。ES执行时间基本无波动。</p>
</blockquote>
</li>
</ul>
<h4 id="Mysql导入ES丢数据原因分析"><a href="#Mysql导入ES丢数据原因分析" class="headerlink" title="Mysql导入ES丢数据原因分析"></a>Mysql导入ES丢数据原因分析</h4><ul>
<li><p>导入过程：同性能测试中的导入方法</p>
</li>
<li><p>原因分析：</p>
<ul>
<li><p>ES中是否含有重复 _id？</p>
<ul>
<li>没有。ES中采用Mysql表的单一主键值做_id，<strong>不会出现重复</strong></li>
</ul>
</li>
<li><p>Mysql数据中是否有row含有特殊字段？（比如string太长、含有特殊编码字符等）</p>
<ul>
<li>没有。二分查找，找到ES中丢失的一些具体数据，数据本身并没有什么特殊</li>
</ul>
</li>
<li><p>分析Mysql数据导入ES的第三方插件源码: elasticsearch-jdbc</p>
<ul>
<li><p>日志中含有出错信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[<span class="number">15</span>:<span class="number">28</span>:<span class="number">15</span>,<span class="number">983</span>][ERROR][org.xbib.elasticsearch.helper.client.BulkTransportClient][elasticsearch[importer][listener][T<span class="comment">#4]] bulk [45] failed with 8020 failed items, failure message = failure in bulk execution:</span></div><div class="line">[<span class="number">0</span>]: index [yqj_info_index_more], type [yqj_info_type_more], id [<span class="number">11598123</span>], message [RemoteTransportException[[Leader][ip:<span class="number">9300</span>][indices:data/write/bulk[s][p]]]; nested: EsRejectedExecutionException[rejected execu</div><div class="line">tion of org.elasticsearch.transport.TransportService$<span class="number">4</span>@<span class="number">7691</span>eb0a on EsThreadPoolExecutor[bulk, queue capacity = <span class="number">50</span>, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@c2f9664[Running, pool size = <span class="number">8</span>, active threads</div><div class="line">= <span class="number">8</span>, queued tasks = <span class="number">50</span>, completed tasks = <span class="number">21008</span>]]];]</div></pre></td></tr></table></figure>
</li>
<li><p>出错原因：导入程序向ES发送数据的速率太快了，超过ES的处理能力。</p>
</li>
<li><p>解决办法：</p>
<ol>
<li>降低导入程序的发送速率 or</li>
<li>增大ES接收数据的线程池数量、缓存队列的size</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="elasticsearch-jdbc-原理分析"><a href="#elasticsearch-jdbc-原理分析" class="headerlink" title="elasticsearch-jdbc 原理分析"></a>elasticsearch-jdbc 原理分析</h4><ul>
<li><p>git地址：<a href="https://github.com/jprante/elasticsearch-jdbc" target="_blank" rel="external">elasticsearch-jdbc</a></p>
</li>
<li><p>处理时序图：</p>
</li>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">title: elasticseach-jdbc时序图</div><div class="line">Runner.main()-&gt; JDBCImport.execute(): 1. 开始向ES导入数据</div><div class="line">Note right of JDBCImport.execute():  启动一个ExecutorServer\n默认线程数为1</div><div class="line">JDBCImport.execute()-&gt; Context.execute(): 2. JDBCImport中构造上下文context\n包含Source(mysql)、Sink(ES)</div><div class="line">Context.execute() -&gt; Context.beforeFetch(): 3. 创建Source、Sink，设置相关参数</div><div class="line">Context.beforeFetch() -&gt; Sink.beforeFetch(): 4. 创建BulkTransportClient，处理fetch前的准备工作\n如：创建ES索引</div><div class="line">Context.beforeFetch() -&gt; Source.beforeFetch(): 5. 这里直接返回</div><div class="line">Context.execute() --&gt; Source.fetch(): 6. 开始fetch数据了</div><div class="line">Source.fetch() -&gt; Source.executeQuery(): 7. 通过JDBC执行SQL</div><div class="line">Source.executeQuery() -&gt; Source.fetch(): 8. 返回SQL结果的迭代器\n</div><div class="line">Source.fetch() -&gt; Source.processRow(): 9. 遍历处理每一行数据\n给数据加上ES的元数据\n如index、type、_id、version等</div><div class="line">Source.processRow() -&gt; Sink.index(): 10. 将数据丢给Sink，构造成IndexRequest，缓存\n当缓存的IndexRequest数量达到一定阈值(10000)\n或经过一定时间后，将这批Request构造成BulkRequest</div><div class="line">Sink.index() -&gt; BulkRequestHandler.execute(): 11. 将BulkRequest经底层Netty发送给ES Server\n若ES处理Failed，log the message</div></pre></td></tr></table></figure>
</li>
<li><p>关键步骤</p>
<ul>
<li><p>JDBCImport中构造上下文context：包含Source(mysql)、Sink(ES)</p>
</li>
<li><p>Sink初始化，创建BulkTransportClient，处理fetch前的准备工作</p>
<ul>
<li><p>如：创建ES索引、设定缓存IndexRequest的阈值，同时处理的BulkRequest最大数量</p>
<blockquote>
<p>Note:  ES处理BulkRequest的线程池默认最大线程数为8，Request缓存队列上线值为50。</p>
<p>若缓存队列满了，还有Request过来就直接抛异常拒绝访问了。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Source中调用JDBC执行SQL，获取SQL结果的迭代器，遍历处理每一行</p>
<ul>
<li>若SQL结果字段名有与ES Control key同名时，就用该字段值替代ES meta。</li>
<li>例如：SQL结果含有字段_id，就将字段值设定为ES存储的文档id</li>
</ul>
</li>
<li><p>Sink将每一行数据封装成IndexRequest，并缓存、处理</p>
<ul>
<li>当缓存的IndexRequest数量达到阈值（10000），将缓存的所有IndexRequest封装成BulkRequest，netty发送给ES Server</li>
<li>当定时线程到期后（默认设定为30秒），就将当前缓存的所有IndexReques封装成BulkRequest，发送给ES Server</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="ES-处理BulkRequest的流程"><a href="#ES-处理BulkRequest的流程" class="headerlink" title="ES 处理BulkRequest的流程"></a>ES 处理BulkRequest的流程</h4><ul>
<li><p>ES启动时，NettyTransport会创建Netty的ServerBootstrap，默认监听端口9300-9400</p>
</li>
<li><p>接收message时，调用ChannelPipeline的ChannelUpStreamHandler依次处理，关键处理类：<code>MessageChannelHandler</code></p>
</li>
<li><p>ES启动时，会注册很多action -&gt; RequestHandler，当收到BulkRequest，调用HandledTransportAction进行处理（action为“data/write/bulk”）</p>
</li>
<li><p>一系列调用后，转到TransportBulkAction.doExecute，根据index分组，若index不存在，会自动创建</p>
</li>
<li><p>TransportBulkAction.executeBulk() 将所有的IndexRequest取出，根据shard分组，构造BulkShardRequest</p>
<ul>
<li>属于同一shard的，都将存储在一起；给IndexRequest分配shard，似乎只是简单轮询</li>
</ul>
</li>
<li><p>遍历BulkShardRequest，调用TransportReplicationAction.doExecute()，Reroute阶段开始（就是将Request发送到Shard所在node）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doExecute</span><span class="params">(Task task, Request request, ActionListener&lt;Response&gt; listener)</span> </span>&#123;</div><div class="line">	<span class="keyword">new</span> ReroutePhase((ReplicationTask) task, request, listener).run();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>从Bulk线程池取出线程处理这个Request，后续存储、建索引过程还未分析</p>
<ul>
<li>默认线程数：8 。Min(处理器核数, 32) </li>
<li>会有个BlockQueue对应这个线程池，若没空闲线程，就将request丢队列，若队列没位置就拒绝这个request</li>
</ul>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>ElasticSearch自身安装简单，但是好些功能（如集群监控、SQL转 DSL Query等）依赖第三方插件<ul>
<li>但是第三方插件稳定性有待商榷。如：Mysql导入ES的插件可能丢失数据（插入失败后未重试）</li>
</ul>
</li>
<li>ElasticSearch能满足实时查询的需求，查询耗时也满足需求，硬件资源消耗也可接受<ul>
<li>一亿级的数据量，count(*)耗时1秒左右</li>
<li>1GB的内存就能顺畅跑起来，执行查询时JVM内存也未见明显波动</li>
</ul>
</li>
</ul>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ul>
<li><a href="http://i.zhcy.tk/blog/elasticsearchyu-solr/" target="_blank" rel="external">搜索引擎选择： Elasticsearch与Solr的调研文档</a></li>
<li><a href="http://www.ctolib.com/topics/79270.html" target="_blank" rel="external">ElasticSearch 5.0 测评以及使用体验</a></li>
<li><a href="http://blog.pengqiuyuan.com/ji-chu-jie-shao-ji-suo-yin-yuan-li-fen-xi/" target="_blank" rel="external">Elasticsearch－基础介绍及索引原理分析</a></li>
<li><a href="http://blog.csdn.net/laoyang360/article/details/52244917" target="_blank" rel="external">Elasticsearch学习，请先看这一篇！</a></li>
<li><a href="http://blog.csdn.net/laoyang360/article/details/52227541" target="_blank" rel="external">Elasticsearch的使用场景深入详解</a></li>
<li><a href="http://www.voidcn.com/blog/bigbigtreewhu/article/p-6323823.html" target="_blank" rel="external">Elasticsearch——Query DSL</a></li>
<li><a href="https://my.oschina.net/jhao104/blog/644909" target="_blank" rel="external">Elasticsearch笔记(一)—Elasticsearch安装配置</a></li>
<li><a href="http://chenghuiz.iteye.com/blog/2310377" target="_blank" rel="external">elasticsearch以及其常用插件安装</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;ElasticSearch调研总结&quot;&gt;&lt;a href=&quot;#ElasticSearch调研总结&quot; class=&quot;headerlink&quot; title=&quot;ElasticSearch调研总结&quot;&gt;&lt;/a&gt;ElasticSearch调研总结&lt;/h3&gt;&lt;h4 id=&quot;Elast
    
    </summary>
    
      <category term="ElasticSearch" scheme="http://yoursite.com/categories/ElasticSearch/"/>
    
    
      <category term="ElasticSearch" scheme="http://yoursite.com/tags/ElasticSearch/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming处理流程源码走读</title>
    <link href="http://yoursite.com/2017/05/03/SparkStreaming%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/"/>
    <id>http://yoursite.com/2017/05/03/SparkStreaming处理流程/</id>
    <published>2017-05-03T00:59:37.651Z</published>
    <updated>2017-05-03T02:17:34.120Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Spark-Streaming处理流程简介"><a href="#Spark-Streaming处理流程简介" class="headerlink" title="Spark Streaming处理流程简介"></a>Spark Streaming处理流程简介</h4><p>​    Spark Streaming 是基于Spark的流式处理框架，会将流式计算分解成一系列短小的批处理作业。Spark Streaming会不停地接收、存储外部数据（如Kafka、MQTT、Socket等），然后每隔一定时间（称之为batch，通常为秒级别的）启动Spark Job来处理这段时间内接收到的数据。</p>
<p>   简单来说，Spark Streaming处理流程为：</p>
<ul>
<li>不停存储外部数据</li>
<li>定期启动Spark Job，处理一个时间段内的数据</li>
</ul>
<h4 id="存储外部数据"><a href="#存储外部数据" class="headerlink" title="存储外部数据"></a>存储外部数据</h4><ul>
<li><p>程序调用流程</p>
<blockquote>
<p>StreamingContext.start()  -&gt;  JobScheduler.start()  -&gt; ReceiverTracker.start()  &amp;&amp; JobGenerator.start()</p>
</blockquote>
<ul>
<li>ReceiverTracker.start() 不停地存储外部数据</li>
<li>JobGenerator.start() 用于处理数据</li>
</ul>
</li>
<li><p>ReceiverTracker</p>
<ul>
<li><p>位于Driver端，用于管理所有的Receiver</p>
<ul>
<li><p>Note：所有ReceiverInputDStream类型的DStream 都对应一个Receiver（用于接收外部数据）</p>
<blockquote>
<p>ReceiverInputDStream.getReceiver() 返回Receiver</p>
</blockquote>
</li>
</ul>
</li>
<li><p>内含ReceivedBlockTracker类型成员</p>
<ul>
<li><p>ReceivedBlockTracker的2个重要方法</p>
<ul>
<li><p>def addBlock(receivedBlockInfo: ReceivedBlockInfo): Boolean</p>
<blockquote>
<p>记录worker发送过来的BlockInfo，存储格式：streamId -&gt; mutable.Queue[ReceivedBlockInfo]</p>
</blockquote>
</li>
<li><p>def allocateBlocksToBatch(batchTime: Time): Unit</p>
<blockquote>
<p>构造Time -&gt; Map[Int, Seq[ReceivedBlockInfo]] ，即构造每个batch对应的流以及Blocks的映射关系</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>构造endpoint，处理ReceiverTrackerLocalMessage类型的本地消息</p>
<ul>
<li>case StartAllReceivers(receivers)</li>
<li>case RestartReceiver(receiver)</li>
<li>case c: CleanupOldBlocks</li>
<li>case UpdateReceiverRateLimit(streamUID, newRate)</li>
<li>case ReportError(streamId, message, error)</li>
</ul>
</li>
<li><p>start()</p>
<ul>
<li><p>启动endpoint，发送StartAllReceivers Message</p>
</li>
<li><p>Note: 在发送StartAllReceivers Message前，执行了runDummySparkJob，用于避免所有receiver被分配到同一个Executor。（设置分区数为50可以确保启动的所有task很小的概率分配到同一host上，而该Spark Job运行结束后，未执行SparkContext.stop，故而BlockManagerMaster中存储的各work的executor信息未清空，可以用于后续需求）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Run the dummy Spark job to ensure that all slaves have registered. This avoids all the</div><div class="line"> * receivers to be scheduled on the same node.</div><div class="line"> *</div><div class="line"> * TODO Should poll the executor number and wait for executors according to</div><div class="line"> * "spark.scheduler.minRegisteredResourcesRatio" and</div><div class="line"> * "spark.scheduler.maxRegisteredResourcesWaitingTime" rather than running a dummy job.</div><div class="line"> */</div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runDummySparkJob</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (!ssc.sparkContext.isLocal) &#123;</div><div class="line">    ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">50</span>, <span class="number">50</span>).map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _, <span class="number">20</span>).collect()</div><div class="line">  &#125;</div><div class="line">  assert(getExecutors.nonEmpty)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>startReceiver()。ReceiverTracker收到自己发送的StartAllReceivers  Message后，对每个receiver执行startReceiver()</p>
<ul>
<li><p>构造RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> receiverRDD: <span class="type">RDD</span>[<span class="type">Receiver</span>[_]] = ssc.sc.makeRDD(<span class="type">Seq</span>(receiver), <span class="number">1</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>指定RDD将执行的function</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> startReceiverFunc: <span class="type">Iterator</span>[<span class="type">Receiver</span>[_]] =&gt; <span class="type">Unit</span> =</div><div class="line">(iterator: <span class="type">Iterator</span>[<span class="type">Receiver</span>[_]]) =&gt; &#123;</div><div class="line">  <span class="keyword">if</span> (!iterator.hasNext) &#123;</div><div class="line">	<span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(</div><div class="line">	  <span class="string">"Could not start receiver as object not found."</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (<span class="type">TaskContext</span>.get().attemptNumber() == <span class="number">0</span>) &#123;</div><div class="line">	<span class="keyword">val</span> receiver = iterator.next()</div><div class="line">	assert(iterator.hasNext == <span class="literal">false</span>)</div><div class="line">	<span class="keyword">val</span> supervisor = <span class="keyword">new</span> <span class="type">ReceiverSupervisorImpl</span>(</div><div class="line">	  receiver, <span class="type">SparkEnv</span>.get, serializableHadoopConf.value, checkpointDirOption)</div><div class="line">	supervisor.start()</div><div class="line">	supervisor.awaitTermination()</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">	<span class="comment">// It's restarted by TaskScheduler, but we want to reschedule it again. So exit it.</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<blockquote>
<p>Note：RDD实际执行ReceiverSupervisorImpl.start()，task失败后重试时将重新调度</p>
</blockquote>
</li>
<li><p>提交Spark Job</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"> <span class="keyword">val</span> future = ssc.sparkContext.submitJob[<span class="type">Receiver</span>[_], <span class="type">Unit</span>, <span class="type">Unit</span>](</div><div class="line">receiverRDD, startReceiverFunc, <span class="type">Seq</span>(<span class="number">0</span>), (_, _) =&gt; <span class="type">Unit</span>, ())</div></pre></td></tr></table></figure>
<blockquote>
<p>至此，Streaming启动了Spark Job，不停的接收外部数据</p>
</blockquote>
</li>
<li><p><strong>ReceiverSupervisorImpl</strong>   直接接收外部数据的关键类，重点分析</p>
<ul>
<li><p><strong>BlockGenerator</strong>。ReceiverSupervisorImpl包含的重要数据成员</p>
<ul>
<li><p>def addData(data: Any): Unit</p>
<blockquote>
<p>将data存入currentBuffer（ArrayBuffer类型）</p>
</blockquote>
</li>
<li><p>updateCurrentBuffer()</p>
<blockquote>
<p>将currentBuffer中的数据构造成Block（使用time做block的uniq id），然后重新构造新的currentBuffer，将Block push 到blocksForPushing 队列（后续有线程不停处理该队列中的block）</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateCurrentBuffer</span></span>(time: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">var</span> newBlock: <span class="type">Block</span> = <span class="literal">null</span></div><div class="line">      synchronized &#123;</div><div class="line">        <span class="keyword">if</span> (currentBuffer.nonEmpty) &#123;</div><div class="line">          <span class="keyword">val</span> newBlockBuffer = currentBuffer</div><div class="line">          currentBuffer = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Any</span>]</div><div class="line">          <span class="keyword">val</span> blockId = <span class="type">StreamBlockId</span>(receiverId, time - blockIntervalMs)</div><div class="line">          listener.onGenerateBlock(blockId)</div><div class="line">          newBlock = <span class="keyword">new</span> <span class="type">Block</span>(blockId, newBlockBuffer)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (newBlock != <span class="literal">null</span>) &#123;</div><div class="line">        blocksForPushing.put(newBlock)  <span class="comment">// put is blocking when queue is full</span></div><div class="line">      &#125;</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
</li>
<li><p>keepPushingBlocks（）</p>
<blockquote>
<p>从blocksForPushing队列中不停地取block，然后处理block。</p>
<p>处理block可分为2个步骤：</p>
<ol>
<li>将block添加进worker本地的BlockManager中</li>
<li>将blockInfo发送到Driver的ReceiverTracker，至此Driver就能感知外部数据了</li>
</ol>
</blockquote>
</li>
<li><p>2个线程之blockIntervalTimer</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> blockIntervalTimer =</div><div class="line">  <span class="keyword">new</span> <span class="type">RecurringTimer</span>(clock, blockIntervalMs, updateCurrentBuffer, <span class="string">"BlockGenerator"</span>)</div></pre></td></tr></table></figure>
<p> 该Timer会启一个线程，周期性的（默认间隔为200ms）调用updateCurrentBuffer</p>
</li>
<li><p>2个线程之blockPushingThread，调用keepPushingBlocks</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> blockPushingThread = <span class="keyword">new</span> <span class="type">Thread</span>() &#123; <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123; keepPushingBlocks() &#125; &#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>ReceiverSupervisorImpl.start()  //回到该方法，继续分析</p>
<ul>
<li><p>onStart()</p>
<blockquote>
<p>BlockGenerator.start()，做好接收外部数据的准备，外部数据会存放在BlockGenerator的currentBuffer中</p>
</blockquote>
</li>
<li><p>onReceiverStart()</p>
<blockquote>
<p>向Driver端的ReceiverTracker注册Receiver</p>
</blockquote>
</li>
<li><p>receiver.onStart()</p>
<blockquote>
<p>即 ReceiverInputDStream.getReceiver().onStart()，用于实际接收外部数据，传递外部数据到BlockGenerator.addData</p>
<p>例如MQTTReceiver.onStart()执行流程: 创建MQTT连接，接收topic，将接收到的message存入BlockGenerator.currentBuffer</p>
</blockquote>
</li>
</ul>
</li>
<li><p>至此，ReceiverTracker 接收外部数据的流程分析完毕，总结为</p>
<blockquote>
<ol>
<li>Driver端的ReceiverTracker 启动Spark Job</li>
<li>worker上调用MQTTReceiver.onStart() 接收外部MQTT数据，并存入BlockGenerator.currentBuffer</li>
<li>BlockGenerator周期性将currentBuffer构造成Block，并同步BlockInfo到Driver的ReceiverTracker</li>
<li>ReceiverTracker能感知Blocks</li>
</ol>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>  ​</p>
<h4 id="定期启动Spark-Job"><a href="#定期启动Spark-Job" class="headerlink" title="定期启动Spark Job"></a>定期启动Spark Job</h4><ul>
<li><p>GenerateJobs</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> timer = <span class="keyword">new</span> <span class="type">RecurringTimer</span>(clock, ssc.graph.batchDuration.milliseconds,</div><div class="line">  longTime =&gt; eventLoop.post(<span class="type">GenerateJobs</span>(<span class="keyword">new</span> <span class="type">Time</span>(longTime))), <span class="string">"JobGenerator"</span>)</div></pre></td></tr></table></figure>
<p>JobGenerator.start() 后，周期性（周期为构造StreamingContext时传入的batch时间参数）调用GenerateJobs。</p>
<p>generateJobs执行步骤：</p>
<ol>
<li><p>ReceiverTracker.allocateBlocksToBatch()</p>
<blockquote>
<p>收集当前batch下，各inputStream产生的BlockInfos</p>
</blockquote>
</li>
<li><p>createBlockRDD(validTime, blockInfos)</p>
<blockquote>
<p>根据上述BlockInfos 创建BlockRDD</p>
</blockquote>
</li>
<li><p>构造jobFunction 与 job</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">getOrCompute(time) <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">Some</span>(rdd) =&gt; &#123;</div><div class="line">    <span class="keyword">val</span> jobFunc = () =&gt; &#123;</div><div class="line">      <span class="keyword">val</span> emptyFunc = &#123; (iterator: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; &#123;&#125; &#125;</div><div class="line">      context.sparkContext.runJob(rdd, emptyFunc)</div><div class="line">    &#125;</div><div class="line">    <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">Job</span>(time, jobFunc))</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="type">None</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p> Job.run() 就是直接调用jobFunc()，运行新的Spark Job，处理rdd</p>
</li>
<li><p>将Job封装成JobSet，丢给线程池（默认一个线程）去实际执行job.run</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">jobSet.jobs.foreach(job =&gt; jobExecutor.execute(<span class="keyword">new</span> <span class="type">JobHandler</span>(job)))</div></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ul>
<ul>
<li><p>总结</p>
<ol>
<li><p>启动独立的Spark Job用于接收外部数据。worker 接收外部数据，周期性(与batch值无关，默认200ms)封装成Block，存入Spark内存中，并将BlockInfo同步给Driver。 </p>
<blockquote>
<p>实际执行Class：BlockGenerator</p>
</blockquote>
</li>
<li><p>Driver周期性（batch值）根据BlockInfos生成BlockRDD，根据RDD构造Spark Job，并执行。</p>
</li>
</ol>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;Spark-Streaming处理流程简介&quot;&gt;&lt;a href=&quot;#Spark-Streaming处理流程简介&quot; class=&quot;headerlink&quot; title=&quot;Spark Streaming处理流程简介&quot;&gt;&lt;/a&gt;Spark Streaming处理流程简介&lt;/
    
    </summary>
    
      <category term="Spark-Streaming" scheme="http://yoursite.com/categories/Spark-Streaming/"/>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="Spark-Streaming" scheme="http://yoursite.com/tags/Spark-Streaming/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2017/05/02/hello-world/"/>
    <id>http://yoursite.com/2017/05/02/hello-world/</id>
    <published>2017-05-02T01:09:27.373Z</published>
    <updated>2017-05-02T01:22:12.520Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
