<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小骑士</title>
  <subtitle>subtitle</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-07-10T03:44:30.696Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>knightyang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark作业提交方式</title>
    <link href="http://yoursite.com/2017/07/10/Spark%20%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F/"/>
    <id>http://yoursite.com/2017/07/10/Spark 作业提交方式/</id>
    <published>2017-07-10T03:42:10.567Z</published>
    <updated>2017-07-10T03:44:30.696Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Spark作业提交方式"><a href="#Spark作业提交方式" class="headerlink" title="Spark作业提交方式"></a>Spark作业提交方式</h3><p>最近碰到个问题，在CDH上采用Spark Streaming方式运行Elasticsearch（ES）相关的处理程序，先后遇到两个JAR冲突：</p>
<ul>
<li>ES的jackson.jar最低需求版本2.4，与CDH的<code>hive-jdbc-1.1.0-cdh5.7.2-standalone.jar</code>里包含的jackson版本冲突</li>
<li>Spark集群默认为JAVA1.7版本，而ES-5.4需要用JAVA1.8</li>
</ul>
<p>解决问题过程中发现，不同的Spark提交作业方式，得有不同的解决办法，不过基本解决思路却是一致的：</p>
<ul>
<li>设置Spark启动Driver、Work时的JAVA_HOME，使之指向JAVA1.8，而非默认指向的JAVA1.7</li>
<li>将ES依赖的jackson.jar放置在JAVA CLASS PATH最前面，以便优先使用。</li>
</ul>
<p>本文主要聊聊常见模式下Spark Driver、Work的JAVA启动参数：</p>
<ul>
<li>local模式</li>
<li>yarn-client模式</li>
<li>yarn-cluster模式</li>
</ul>
<h3 id="local模式"><a href="#local模式" class="headerlink" title="local模式"></a>local模式</h3><p>local模式下，spark driver、worker都在本地线程池里运行，位于同一个  JAVA进程。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">spark-submit \</div><div class="line">  --master local[2] \</div><div class="line">  --jars jackson-core-2.8.6.jar,my-dependent.jar \</div><div class="line">  --class clife.data.spark.ESUpdate  </div><div class="line">  my-main.jar</div></pre></td></tr></table></figure>
<ol>
<li><p>调用<code>SPARK_HOME/bin/spark-submit</code>脚本，转向spark-class</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">exec "$&#123;SPARK_HOME&#125;"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"</div></pre></td></tr></table></figure>
</li>
<li><p><code>SPARK_HOME/bin/spark-class</code> 加载spark env 变量后，启动JVM执行<code>org.apache.spark.launcher.Main</code> 获取后续执行命令，最后执行该命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">build_command() &#123;</div><div class="line">  "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"</div><div class="line">  printf "%d\0" $?</div><div class="line">&#125;</div><div class="line"></div><div class="line">CMD=()</div><div class="line">while IFS= read -d '' -r ARG; do</div><div class="line">  CMD+=("$ARG")</div><div class="line">done &lt; &lt;(build_command "$@")</div><div class="line"></div><div class="line">CMD=("$&#123;CMD[@]:0:$LAST&#125;")</div><div class="line">exec "$&#123;CMD[@]&#125;"</div></pre></td></tr></table></figure>
</li>
<li><p>运行<code>org.apache.spark.launcher.Main</code>，输出JAVA命令。</p>
<ul>
<li><strong>会先将SPARK_CLASSPATH变量对应的值加入JAVA CLASSPATH</strong>，随后才是SPARK_CONF、SPARK_JAR、lib等等</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function">List&lt;String&gt; <span class="title">buildClassPath</span><span class="params">(String appClassPath)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">  String sparkHome = getSparkHome();</div><div class="line"></div><div class="line">  List&lt;String&gt; cp = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line">  addToClassPath(cp, getenv(<span class="string">"SPARK_CLASSPATH"</span>));</div><div class="line">  addToClassPath(cp, appClassPath);</div><div class="line"></div><div class="line">  addToClassPath(cp, getConfDir());</div></pre></td></tr></table></figure>
<ul>
<li><p>输出命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span>&#123;JAVA_HOME&#125;/bin/java \</div><div class="line">  -cp /CDH-5.7.2-1.cdh5.7/jars/* \</div><div class="line">  -XX:MaxPermSize=256m \</div><div class="line">  org.apache.spark.deploy.SparkSubmit \</div><div class="line">  --master local[2] \</div><div class="line">  --class my.main.class.name \</div><div class="line">  ...</div><div class="line">  my-main.jar</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><code>SPARK_HOME/bin/spark-class</code>运行上述命令，正式执行Spark程序</p>
</li>
<li><p>jar包冲突的解决办法：</p>
<ul>
<li>解决jackson冲突:   <code>export SPARK_CLASSPATH=.:jackson-core-2.8.6.jar</code>，会将该jar放在JVM CLASS_PATH最前面，优先加载</li>
<li>解决java版本冲突：本地export JAVA_HOME=jdk1.8</li>
</ul>
</li>
</ol>
<h4 id="yarn-client模式"><a href="#yarn-client模式" class="headerlink" title="yarn-client模式"></a>yarn-client模式</h4><p>会在本地启动spark driver，向yarn申请资源，集群运行task。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">--master yarn \</div><div class="line">--deploy-mode client \</div></pre></td></tr></table></figure>
<h6 id="解决Driver端的冲突"><a href="#解决Driver端的冲突" class="headerlink" title="解决Driver端的冲突"></a>解决Driver端的冲突</h6><p>JAVA版本冲突：</p>
<ul>
<li>由于client模式的Driver运行在本地，可以本地export JAVA_HOME=jdk1.8，即可解决java版本问题。</li>
</ul>
<p>Jackson Jar冲突：</p>
<ul>
<li><p>脚本运行流程上和local模式一致，在执行<code>org.apache.spark.deploy.SparkSubmit</code>时，与local模式出现不同。简单来说，就是根据输入的不同参数，生成不同的启动Driver的上下文。</p>
</li>
<li><p>设置<code>--verbose</code> 可打印在<code>SparkSubmit</code>里通过反射执行<code>childMainClass</code> 时的参数。</p>
</li>
<li><p>重点关注childClasspath，会将其首先加入classloader里:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (verbose) &#123;</div><div class="line">  printStream.println(<span class="string">s"Main class:\n<span class="subst">$childMainClass</span>"</span>)</div><div class="line">  printStream.println(<span class="string">s"Arguments:\n<span class="subst">$&#123;childArgs.mkString("\n")&#125;</span>"</span>)</div><div class="line">  printStream.println(<span class="string">s"System properties:\n<span class="subst">$&#123;sysProps.mkString("\n")&#125;</span>"</span>)</div><div class="line">  printStream.println(<span class="string">s"Classpath elements:\n<span class="subst">$&#123;childClasspath.mkString("\n")&#125;</span>"</span>)</div><div class="line">  printStream.println(<span class="string">"\n"</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">val</span> loader =</div><div class="line">  <span class="keyword">if</span> (sysProps.getOrElse(<span class="string">"spark.driver.userClassPathFirst"</span>, <span class="string">"false"</span>).toBoolean) &#123;</div><div class="line">    <span class="keyword">new</span> <span class="type">ChildFirstURLClassLoader</span>(<span class="keyword">new</span> <span class="type">Array</span>[<span class="type">URL</span>](<span class="number">0</span>),</div><div class="line">      <span class="type">Thread</span>.currentThread.getContextClassLoader)</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">new</span> <span class="type">MutableURLClassLoader</span>(<span class="keyword">new</span> <span class="type">Array</span>[<span class="type">URL</span>](<span class="number">0</span>),</div><div class="line">      <span class="type">Thread</span>.currentThread.getContextClassLoader)</div><div class="line">  &#125;</div><div class="line"><span class="type">Thread</span>.currentThread.setContextClassLoader(loader)</div><div class="line"></div><div class="line"><span class="keyword">for</span> (jar &lt;- childClasspath) &#123;</div><div class="line">  addJarToClasspath(jar, loader)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Classpath elements:</div><div class="line">file:/home/my/my-main.jar</div><div class="line">file:/home/my/jackson-core-2.8.6.jar</div></pre></td></tr></table></figure>
</li>
<li><p>而client模式下，一般通过 <code>--jars</code>方式提交jar包，会加载jar并传达给<code>childClasspath</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (deployMode == <span class="type">CLIENT</span>) &#123;</div><div class="line">  childMainClass = args.mainClass</div><div class="line">  <span class="keyword">if</span> (isUserJar(args.primaryResource)) &#123;</div><div class="line">    childClasspath += args.primaryResource</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (args.jars != <span class="literal">null</span>) &#123; childClasspath ++= args.jars.split(<span class="string">","</span>) &#125;</div><div class="line">  <span class="keyword">if</span> (args.childArgs != <span class="literal">null</span>) &#123; childArgs ++= args.childArgs &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>这样在Driver端的jar冲突就解决了。</p>
</li>
</ul>
<h6 id="解决Executor-Task的冲突"><a href="#解决Executor-Task的冲突" class="headerlink" title="解决Executor Task的冲突"></a>解决Executor Task的冲突</h6><p>  <code>ApplicationMaster</code>用于Spark向Yarn申请资源，<code>ExecutorRunnable</code>会生成Executor JVM启动时的参数（SPARK默认配置下会打印）。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">val commands = prepareCommand(masterAddress, slaveId, hostname, executorMemory, executorCores,</div><div class="line">  appId, localResources)</div><div class="line"></div><div class="line">logInfo(s"""</div><div class="line">  |===============================================================================</div><div class="line">  |YARN executor launch context:</div><div class="line">  |  env:</div><div class="line">  |$&#123;env.map &#123; case (k, v) =&gt; s"    $k -&gt; $v\n" &#125;.mkString&#125;</div><div class="line">  |  command:</div><div class="line">  |    $&#123;commands.mkString(" ")&#125;</div><div class="line">  |===============================================================================</div><div class="line">  """.stripMargin)</div></pre></td></tr></table></figure>
<p>JAVA版本冲突：</p>
<ul>
<li><p>会从环境变量里获取JAVA_HOME</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> commands = prefixEnv ++ <span class="type">Seq</span>(</div><div class="line">      <span class="type">YarnSparkHadoopUtil</span>.expandEnvironment(<span class="type">Environment</span>.<span class="type">JAVA_HOME</span>) + <span class="string">"/bin/java"</span>,</div><div class="line">      <span class="string">"-server"</span>,</div></pre></td></tr></table></figure>
</li>
<li><p>Executor启动前的环境（设置环境变量、class-path、相关参数）也是在<code>ExecutorRunnable</code>里设置的</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> env = prepareEnvironment(container) <span class="comment">//准备Container启动的环境(环境变量、class-path、jvm参数、Executor参数)</span></div><div class="line"></div><div class="line">sparkConf.getExecutorEnv.foreach &#123; <span class="keyword">case</span> (key, value) =&gt;</div><div class="line">      <span class="comment">// This assumes each executor environment variable set here is a path</span></div><div class="line">      <span class="comment">// This is kept for backward compatibility and consistency with hadoop</span></div><div class="line">      <span class="type">YarnSparkHadoopUtil</span>.addPathToEnvironment(env, key, value)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">  <span class="comment">/** Get all executor environment variables set on this SparkConf */</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getExecutorEnv</span></span>: <span class="type">Seq</span>[(<span class="type">String</span>, <span class="type">String</span>)] = &#123;</div><div class="line">    <span class="keyword">val</span> prefix = <span class="string">"spark.executorEnv."</span></div><div class="line">    getAll.filter&#123;<span class="keyword">case</span> (k, v) =&gt; k.startsWith(prefix)&#125;</div><div class="line">          .map&#123;<span class="keyword">case</span> (k, v) =&gt; (k.substring(prefix.length), v)&#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addPathToEnvironment</span></span>(env: <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>], key: <span class="type">String</span>, value: <span class="type">String</span>): <span class="type">Unit</span> =   &#123;</div><div class="line">    <span class="keyword">val</span> newValue = <span class="keyword">if</span> (env.contains(key)) &#123; env(key) + getClassPathSeparator  + value &#125; <span class="keyword">else</span>       value</div><div class="line">    env.put(key, newValue)</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
</li>
<li><p>解决办法：提交Spark作业时指定<code>--conf spark.executorEnv.JAVA_HOME=/JAVA1.8-dir/</code>，即可复写默认JAVA_HOME</p>
</li>
</ul>
<p>Jackson.jar冲突:</p>
<ul>
<li><p><code>ExecutorRunnable</code>设置class-path，加载顺序为：</p>
<ul>
<li><code>spark.executor.extraClassPath</code></li>
<li><code>Environment.PWD</code></li>
<li><code>spark.yarn.jar</code>  或   <code>SPARK_JAR</code></li>
<li><code>HadoopClasspath</code></li>
<li><code>SPARK_DIST_CLASSPATH</code></li>
</ul>
</li>
<li><p>CDH的<code>hive-jdbc-1.1.0-cdh5.7.2-standalone.jar</code>(含冲突jackson)位于<code>HadoopClasspath</code></p>
</li>
<li><p>解决办法：提交Spark作业时指定<code>--conf spark.executor.extraClassPath=/jackson.jar</code></p>
<p>​</p>
</li>
</ul>
<h4 id="yarn-cluster模式"><a href="#yarn-cluster模式" class="headerlink" title="yarn-cluster模式"></a>yarn-cluster模式</h4><p>向yarn申请资源，运行driver、task。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">--master yarn \</div><div class="line">--deploy-mode cluster \</div></pre></td></tr></table></figure>
<p>命令行提交spark作业时，<code>spark-submit</code>会自动JVM，执行main-class: <code>org.apache.spark.deploy.Client</code>，然后向Yarn申请资源，submitApplication，运行Driver。</p>
<h6 id="解决Driver端冲突"><a href="#解决Driver端冲突" class="headerlink" title="解决Driver端冲突"></a>解决Driver端冲突</h6><p><code>yarn.Client.createContainerLaunchContext()</code> 生成Driver提交环境。</p>
<p>解决java版本冲突：</p>
<ul>
<li>类似yarn-client模式下运行work，JAVA_HOME也是从环境变量中取，<code>spark.yarn.appMasterEnv.JAVA_HOME</code>可以覆写。</li>
</ul>
<p>解决jackson.jar冲突：</p>
<ul>
<li>类似yarn-client模式下运行work，设置<code>spark.driver.extraClassPath=/jackson.jar</code>解决</li>
</ul>
<h6 id="解决Executor-Task端冲突"><a href="#解决Executor-Task端冲突" class="headerlink" title="解决Executor Task端冲突"></a>解决Executor Task端冲突</h6><ul>
<li>解决方法同yarn-client模式下运行的work</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>在不能升级CDH集群配置前提下，解决Spark Jar冲突、JAVA版本不一致的方法其实很简单：</p>
<ul>
<li>定位Driver、Work的JVM启动命令</li>
<li>将程序需要的Jar放置于class-path最前面</li>
<li>改写默认JAVA_HOME</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Spark作业提交方式&quot;&gt;&lt;a href=&quot;#Spark作业提交方式&quot; class=&quot;headerlink&quot; title=&quot;Spark作业提交方式&quot;&gt;&lt;/a&gt;Spark作业提交方式&lt;/h3&gt;&lt;p&gt;最近碰到个问题，在CDH上采用Spark Streaming方式运行
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="spark-submit" scheme="http://yoursite.com/tags/spark-submit/"/>
    
  </entry>
  
  <entry>
    <title>用Elasticsearch处理SQL查询</title>
    <link href="http://yoursite.com/2017/06/26/%E7%94%A8Elasticsearch%E5%A4%84%E7%90%86SQL%E6%9F%A5%E8%AF%A2/"/>
    <id>http://yoursite.com/2017/06/26/用Elasticsearch处理SQL查询/</id>
    <published>2017-06-26T09:59:54.584Z</published>
    <updated>2017-06-26T10:00:35.962Z</updated>
    
    <content type="html"><![CDATA[<h2 id="用Elasticsearch处理SQL查询"><a href="#用Elasticsearch处理SQL查询" class="headerlink" title="用Elasticsearch处理SQL查询"></a>用Elasticsearch处理SQL查询</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>最近碰到个需求：业务方mysql单机查询耗时无法满足需求、部分sql查询不出来，希望能提供个实时查询框架解决这个问题，最好是分布式的，好扩展嘛。（PS：最开始产品经理聊时，提到mysql 库表太大、单表count(<em>)都不能出结果。目测单表数据量估计快10亿了，结果后来和业务后台开发核对细节时，发现单表数据量最多也就百万级，最慢sql也就耗时几秒，哪儿有撒count(</em>)不出结果的，也是醉了，还是得多和搞技术的交流啊。真实需求是百万级数据量需要查询耗时毫秒级）</p>
<p>调研了各种实时查询框架：Elasticsearch、druid、phoenix、solr、impala，最终选定了Elasticsearch，<strong>版本5.4.0</strong>。原因简单来说：ES更简单、更快，当然缺点是对join的支持也不好、没有分区的概念。</p>
<p>本文主要聊聊如何用ES处理SQL业务，碰到的问题、解决版本。不会涉及ES的基本概念。</p>
<p>主要聊聊：</p>
<ul>
<li>ES文档结构设计</li>
<li>Mysql数据如何导入ES</li>
<li>写ES Query DSL中碰到的问题</li>
</ul>
<h3 id="文档结构的设计"><a href="#文档结构的设计" class="headerlink" title="文档结构的设计"></a>文档结构的设计</h3><p>  在文档结构设计上尝试了2种方案：</p>
<h5 id="方案1：将ES当关系数据库使，数据库表结构与ES文档结构一一对应。-参考系列文章：把Elasticsearch-当数据库使"><a href="#方案1：将ES当关系数据库使，数据库表结构与ES文档结构一一对应。-参考系列文章：把Elasticsearch-当数据库使" class="headerlink" title="方案1：将ES当关系数据库使，数据库表结构与ES文档结构一一对应。 参考系列文章：把Elasticsearch 当数据库使"></a>方案1：将ES当关系数据库使，数据库表结构与ES文档结构一一对应。 参考系列文章：<a href="https://segmentfault.com/a/1190000004433446" target="_blank" rel="external">把Elasticsearch 当数据库使</a></h5><p>简单说，就是对于数据库中的每一个表，都在ES中创建一个type（类似数据库里的表），ES type里的feild 名称/类型  与数据库表的字段名/类型 保持一致。</p>
<p>建表简单、mysql数据导入ES也简单，但是将SQL转换成ES Query DSL时 碰到一系列问题：</p>
<ul>
<li><p>问题一：ES Join支持不好</p>
<ul>
<li><p><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/relations.html" target="_blank" rel="external">ES处理join的几种方式</a>（select * from tbl1 join tbl2 on tbl1.id = tbl2.id;）</p>
<ul>
<li><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/application-joins.html" target="_blank" rel="external">应用层联接</a><ul>
<li>ES转换成<ul>
<li>执行 select id, * from tbl1; 结果存数组变量tbl1_id_array</li>
<li>执行 select * from tbl2 where id in [tbl1_id_array]</li>
<li>结果再拼接</li>
</ul>
</li>
<li>适用于tbl1的数据量很小 </li>
</ul>
</li>
<li><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/denormalization.html" target="_blank" rel="external">表字段冗余</a><ul>
<li>ES 在建表时，将table1的信息冗余存入tabl2</li>
</ul>
</li>
<li><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/nested-objects.html" target="_blank" rel="external">嵌套对象</a> <ul>
<li>比如SQL里 一个表存储所有文章，一个表存储所有评论，评论表中存储文章id，通过join能查找一篇文章的所有评论</li>
<li>ES 可以只创建一个表，每篇文章里存储所有的评论，来规避join</li>
</ul>
</li>
<li><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/parent-child.html" target="_blank" rel="external">父-子关系文档</a><ul>
<li>ES 需要在设计表的时候  就指定好2表之间的关系</li>
<li>可以处理2张表都很大的情况，但是需要在创建表的时候就建立join关系，不大实用</li>
</ul>
</li>
</ul>
</li>
<li><p>其他资料</p>
<ul>
<li><a href="https://segmentfault.com/a/1190000004468130" target="_blank" rel="external">把Elasticsearch当数据库使：Join</a> </li>
</ul>
</li>
<li><p>Join总结</p>
<ul>
<li>ES不太适合关系数据库的join。ES JOin只会返回单表的数据，Mysql Join 会返回左右2表的数据。另外join又分为left join、right join、inner join等等，ES得用户处理这些join逻辑，不大方便。</li>
</ul>
</li>
<li><p>问题二：Union ALL、子查询不支持</p>
</li>
</ul>
</li>
</ul>
<h5 id="方案2：将Mysql的库表结构转换成ES容易处理的文档结构"><a href="#方案2：将Mysql的库表结构转换成ES容易处理的文档结构" class="headerlink" title="方案2：将Mysql的库表结构转换成ES容易处理的文档结构"></a>方案2：将Mysql的库表结构转换成ES容易处理的文档结构</h5><p> 简单说，就是通过新的文档结构查询时，不再需要join、union all、子查询了。</p>
<p> 不过这个设计过程就比较痛苦了，需要比较深入的了解业务库表结构，各种摸爬滚打后，将库表结构转换成2种ES文档结构：</p>
<ul>
<li><p>由单个mysql表构成</p>
<ul>
<li><p>普通文档</p>
<ol>
<li>ES字段与mysql字段的名字、类型保持一致</li>
<li><p>若mysql表含有主键，就将主键作为ES的_id</p>
<blockquote>
<p>例如: mysql table 主键为(col1, col2)，则可将col1_col2作为ES的_id</p>
</blockquote>
</li>
<li><p>若mysql表里没有主键，则视情况由ES自动生成_id 或者用别的方式</p>
</li>
</ol>
</li>
<li>父文档<ul>
<li>mysql里必须含有主键，并将主键值作为ES文档的_id值</li>
</ul>
</li>
<li>子文档<ul>
<li>mysql表里必须要有父文档_id对应的字段<blockquote>
<p>例如 父文档为school，_id为”school_id”对应的值，那子文档class里必须存有字段”school_id”</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>由多个mysql表构成</p>
<ul>
<li>确定唯一的main table。其余表数据会与该表有关联。</li>
<li>确定其余副表是作为ES的nested common field 还是 nested array field。即主表的一行记录在副本中是最多有一行对应数据、还是多行对应数据。</li>
</ul>
</li>
<li><p>例子：ES device表的设计过程</p>
<ul>
<li><p>确定由哪些表构成。device表包含mysql的七张表:      </p>
<ul>
<li>tb_device_mac (device_id, product_id, status)</li>
<li>tb_device_mac_history (device_id, mac_address, status)</li>
<li>tb_device_auth (devce_id, user_id, auth_time)</li>
<li>tb_device_auth_history (devce_id, user_id, auth_time)</li>
<li>tb_device_bind (device_id, bind_time)</li>
<li>tb_device_bind_history (device_id, bind_time, unbind_time)</li>
<li>tb_product (product_id, product_name)</li>
</ul>
</li>
<li><p>确定主表: tb_device_mac</p>
</li>
<li>确定nested common field 对应的副表: tb_product</li>
<li>表融合: 相似结构的表合并成一个字段: 如 tb_device_mac 和 tb_device_mac_history 在ES里用一个字段存储，即tb_device_mac 会冗余存储</li>
<li><p>device ES 表结构</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&quot;properties&quot;: &#123;</div><div class="line">    &quot;device_id&quot; : &#123;&#125;,  // 来自主表tb_device_mac的字段</div><div class="line">    &quot;mac_address&quot;: &#123;&#125;,</div><div class="line">    &quot;status&quot;: &#123;&#125;,</div><div class="line">    &quot;macs&quot;: &#123; // 包含来自tb_device_mac 和 tb_device_mac_history的字段</div><div class="line">        &quot;type&quot;: &quot;nested&quot;,</div><div class="line">        &quot;properties&quot;: &#123;</div><div class="line">            &quot;device_id&quot; : &#123;&#125;,</div><div class="line">            &quot;product_id&quot;: &#123;&#125;,</div><div class="line">            &quot;status&quot;: &#123;&#125;,</div><div class="line">            &quot;is_history&quot;: &#123;&quot;type&quot;: &quot;boolean&quot;&#125; // 新增字段</div><div class="line">        &#125;</div><div class="line">    &#125;,</div><div class="line">    &quot;auths&quot;: &#123;&#125; // 存储来自tb_device_auth 和 tb_dvice_auth_history的数据</div><div class="line">    &quot;binds&quot;: &#123;&#125; // 存储来自tb_device_bind 和 tb_device_bind_history的数据</div><div class="line">    &quot;product&quot;: &#123;&#125; //存储来自tb_product的数据</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>device 数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;device_id&quot;: 1,</div><div class="line">    &quot;mac_address&quot;: &quot;ABCDEFG&quot;,</div><div class="line">    &quot;status&quot;: 0,</div><div class="line">    &quot;macs&quot;: [ // tb_device_mac 与 (tb_device_mac union tb_device_mac_history) 是一对多关系，因此用数组存储</div><div class="line">        &#123;  // 来自tb_device_mac的数据会冗余存储一份在这里，便于处理tb_device_mac union all tb_device_mac_history 的数据</div><div class="line">            &quot;device_id&quot;: 1,</div><div class="line">            &quot;product_id&quot;: 12,</div><div class="line">            &quot;status&quot;: 0,</div><div class="line">            &quot;is_history&quot;: false</div><div class="line">        &#125;，</div><div class="line">        &#123;</div><div class="line">            &quot;device_id&quot;: 1,</div><div class="line">            &quot;product_id&quot;: 12,</div><div class="line">            &quot;status&quot;: 1,</div><div class="line">            &quot;is_history&quot;: true</div><div class="line">        &#125;</div><div class="line">    ],</div><div class="line">    &quot;binds&quot;: [&#123;...&#125;],</div><div class="line">    &quot;auths&quot;: [],</div><div class="line">    &quot;product&quot;: &#123; // tb_product与tb_device_mac是一对一关系，因此这里不用数组存储</div><div class="line">        &quot;product_id&quot;: 12,</div><div class="line">        &quot;product_nam&quot;: &quot;hello&quot;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>文档设计总结</p>
<ul>
<li>虽然方案2相比于方案1，在设计难度上更高，但是能有效的解决ES对join、uion all、子查询支持不好的问题。<blockquote>
<p>Tips: 方案2将之前SQL join、子查询、union all设计到的表融合到ES的同一个文档中，能用简单Query DSL语句实现之前的复杂SQL</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h4 id="数据导入ES"><a href="#数据导入ES" class="headerlink" title="数据导入ES"></a>数据导入ES</h4><p>应业务方需求，现网数据的写入、更新方式不变。也就是依旧还是会把数据更新到Mysql，不会直接写ES，而仅仅是查询ES。所以，这边的有个机制，把Mysql上的更新操作实时的同步到ES。</p>
<ul>
<li>这里采用了阿里开源的canal监听mysql binlog</li>
<li>将mysql insert、update、delete涉及的数据发Kafka，</li>
<li>写了个ES程序从Kafka读数据，</li>
<li>封装成ES的IndexRequest/UpdateRequest/DeleteRequest，批量处理(构造BulkRequest，设置批量阈值为1000)。</li>
</ul>
<p>ES同步程序里，会将mysql的部分insert请求，转换成ES的UpdateRequest，例如前文提到的文档结构device，mysql insert bind 会转换成 ES  update device.binds 。这个过程中竟然还导致ES服务器进程出现OutOfMemory，导致服务器进程异常退出。</p>
<p>OutOfMemory原因分析：</p>
<ul>
<li>ES服务器采用默认JVM配置，JVM内存为2G，<code>-XX:+HeapDumpOnOutOfMemoryError</code>会自动dump内存，生成hprof文件。</li>
<li><p>分析发现，85%以上的内存被BulkShardRequest.iterms占用，iterms里竟然大部分都是IndexRequest，而不是UpdateRequest!!!</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">Class Name                                                                                        | Shallow Heap | Retained Heap</div><div class="line">---------------------------------------------------------------------------------------------------------------------------------</div><div class="line">org.elasticsearch.action.bulk.BulkShardRequest @ 0xbf1945d0                                       |           64 | 1,654,253,216</div><div class="line">|- items org.elasticsearch.action.bulk.BulkItemRequest[1000] @ 0xbf194610                         |        4,016 | 1,654,253,152</div><div class="line">|  |- [736] org.elasticsearch.action.bulk.BulkItemRequest @ 0x8b9cb678                            |           32 |     1,968,936</div><div class="line">|  |  |- request org.elasticsearch.action.index.IndexRequest @ 0x8b7eab88                         |          120 |     1,968,792</div><div class="line">|  |  |  |- source org.elasticsearch.common.bytes.PagedBytesReference @ 0x8b9cb5d8                |           32 |     1,968,600</div><div class="line">|  |  |  |- opType org.elasticsearch.action.DocWriteRequest$OpType @ 0xafba9758                   |           32 |            88</div><div class="line">|  |  |  |- index java.lang.String @ 0xbf0bccd0  bigdata-realtime-v1                              |           24 |            80</div><div class="line">|  |  |  |- type java.lang.String @ 0xbf0bcd60  device                                            |           24 |            56</div><div class="line">|  |  |  |- id java.lang.String @ 0xbf0bcd98  1006685                                             |           24 |            56</div><div class="line">|  |  |  &apos;- Total: 13 entries                                                                     |              |              </div><div class="line">|  |  |- primaryResponse org.elasticsearch.action.bulk.BulkItemResponse @ 0x8b9cb698              |           32 |           112</div><div class="line">|  |  |- &lt;class&gt; class org.elasticsearch.action.bulk.BulkItemRequest @ 0xb1e7ded0                 |            8 |             8</div><div class="line">|  |  &apos;- Total: 3 entries                                                                         |              |              </div><div class="line">|  |- [737] org.elasticsearch.action.bulk.BulkItemRequest @ 0x8bbac2e0                            |           32 |     1,968,936</div><div class="line">|  |- [738] org.elasticsearch.action.bulk.BulkItemRequest @ 0x8bd8ce88                            |           32 |     1,968,936</div><div class="line"></div><div class="line">---------------------------------------------------------------------------------------------------------------------------------</div></pre></td></tr></table></figure>
</li>
<li><p>查看ES源码，UpdateRequest会转换成IndexRequest，而由于同一个设备(device)文档，内嵌的数据量高达3w多(device.binds为含有3w多元素的数组)，导致单个文档太大（3MB），不巧一个bulk里1000个UpdateRequest都是涉及这种文档的，最终导致OOM</p>
</li>
<li>解决办法：<ul>
<li>调大ES JVM内存</li>
<li>让业务查看数据是否引入脏数据，device.binds元素个数3000以内为正常</li>
</ul>
</li>
</ul>
<h4 id="SQL转换成Query-DSL"><a href="#SQL转换成Query-DSL" class="headerlink" title="SQL转换成Query DSL"></a>SQL转换成Query DSL</h4><p>这块改写比较简单，目前只碰到一个问题：</p>
<ul>
<li>ES 对于Count(distinct col) 采用基数估算，结果不精确，而业务方需要精确值<ul>
<li>解决办法：改写成select col from tbl group by col，客户端再count</li>
</ul>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>杀鸡用了牛刀，可惜没用上ES核心功能（搜索引擎）。后续会考虑用ELK实现日志实时搜索框架。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;用Elasticsearch处理SQL查询&quot;&gt;&lt;a href=&quot;#用Elasticsearch处理SQL查询&quot; class=&quot;headerlink&quot; title=&quot;用Elasticsearch处理SQL查询&quot;&gt;&lt;/a&gt;用Elasticsearch处理SQL查询&lt;/
    
    </summary>
    
      <category term="ElasticSearch" scheme="http://yoursite.com/categories/ElasticSearch/"/>
    
    
      <category term="ElasticSearch" scheme="http://yoursite.com/tags/ElasticSearch/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch调研总结</title>
    <link href="http://yoursite.com/2017/05/10/ElasticSearch%E8%B0%83%E7%A0%94%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2017/05/10/ElasticSearch调研总结/</id>
    <published>2017-05-10T09:37:53.142Z</published>
    <updated>2017-05-10T09:39:22.961Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ElasticSearch调研总结"><a href="#ElasticSearch调研总结" class="headerlink" title="ElasticSearch调研总结"></a>ElasticSearch调研总结</h3><h4 id="ElasticSearch简介"><a href="#ElasticSearch简介" class="headerlink" title="ElasticSearch简介"></a>ElasticSearch简介</h4><p>​    Elasticsearch 是一个分布式可扩展的<strong>实时</strong>搜索和分析引擎,一个建立在全文搜索引擎 Apache Lucene(TM) 基础上的搜索引擎。ElasticSearch可以做的事儿:</p>
<ul>
<li>分布式的实时文件存储，并为每一个字段建立索引，默认存储在本地磁盘</li>
<li>实时分析的分布式搜索引擎</li>
<li>集群规模可以动态扩展，具备分布式的基本要求：动态扩展、容错、负载均衡</li>
</ul>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li><p>Cluster (集群)</p>
</li>
<li><p>Node (节点)</p>
<blockquote>
<p>每个节点上运行一个ElasticSearch实例，节点启动后会自动广播查询，配置信息中cluster.name值相同的节点会构造成一个集群</p>
</blockquote>
</li>
<li><p>Shard (分片)</p>
<blockquote>
<p>类似HDFS中的block，会将大容量的数据切分成多个Shard，不同的Shard可以分散存储在集群上不同的节点</p>
</blockquote>
</li>
<li><p>Replia (副本)</p>
<blockquote>
<p>一个Replia是一个Shard的精准复制，每个Shard可含有0个或多个Replia。用于容错、并发查询</p>
</blockquote>
</li>
<li><p>面向文档</p>
<p>​    ElasticSearch是面向文档存储的，基本的存储单位就是文档，一条记录就是一个文档，文档格式统一为JSON格式，例如：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"name"</span> :     <span class="string">"John"</span>,</div><div class="line">    <span class="attr">"sex"</span> :      <span class="string">"Male"</span>,</div><div class="line">    <span class="attr">"age"</span> :      <span class="number">25</span>,</div><div class="line">    <span class="attr">"birthDate"</span>: <span class="string">"1990/05/01"</span>,</div><div class="line">    <span class="attr">"about"</span> :    <span class="string">"I love to go rock climbing"</span>,</div><div class="line">    <span class="attr">"interests"</span>: [ <span class="string">"sports"</span>, <span class="string">"music"</span> ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>index (索引)、type (类型)、id </p>
<p>ES中通过index/type/id 来唯一标示一个文档。与Mysql概念对比：</p>
<p>| Mysql         | ES                  |<br>| ————- | ——————- |<br>| database(数据库) | index(索引)           |<br>| table(表)      | type(类型)            |<br>| row(行)        | 文档                  |<br>| column(列)     | field               |<br>| schema        | mapping             |<br>| index         | everything is index |<br>| sql           | Query DSL           |</p>
</li>
<li><p>Query DSL</p>
<p>Mysql 中采用SQL语句进行查询，ES中统一采用Query DSL查询，详情可参考：<a href="http://www.voidcn.com/blog/bigbigtreewhu/article/p-6323823.html" target="_blank" rel="external">Elasticsearch——Query DSL</a>。<code>select * from bank;</code> 示例：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">curl -XPOST 'localhost:9200/bank/_search?pretty' -d '</div><div class="line">&#123;</div><div class="line">  "query": &#123; "match_all": &#123;&#125; &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h4><ul>
<li><p>版本选择与下载、解压，<a href="[http://www.elasticsearch.org/download/](http://www.elasticsearch.org/download/">下载地址</a> )</p>
<ul>
<li><strong>Note:  调研版本选择的是2.4.1，而非最新版本5.*</strong>(版本号2.4之后就直接是5.* )，原因：<ul>
<li>5.<em> 要求JAVA8，5.</em> 官方插件也要求JAVA8</li>
<li>很多常用的第三方插件（bigdesk、head、sql、jdbc-sql）还未支持5.*</li>
<li>2.4 与 5.* 性能实测差异不大，详见<a href="http://www.ctolib.com/topics/79270.html" target="_blank" rel="external">ElasticSearch 5.0 测评以及使用体验</a></li>
</ul>
</li>
</ul>
</li>
<li><p>配置所有节点，将$ES_HOME/elasticsearch.yml中cluster.name 修改成相同值</p>
</li>
<li><p>$ES_HOME/bin/elasticsearch</p>
<blockquote>
<p>所有节点上执行执行，就启动了ES。会自动广播组成集群。</p>
</blockquote>
<p>JVM默认参数：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">-Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true</div></pre></td></tr></table></figure>
</li>
<li><p>集群访问地址：host:9200</p>
<blockquote>
<p><a href="http://200.200.200.64:9200/" target="_blank" rel="external">http://200.200.200.64:9200/</a></p>
</blockquote>
</li>
<li><p>插件安装</p>
<ul>
<li><p>Note：ES安装包只提供了最基本的功能：本地分布式存储数据、创建索引、提供查询服务等。其余一些额外功能则需要第三方插件支持，如：集群状态监控、SQL转DSL Query、集群性能监控、Mysql数据导入ES等。详见：<a href="http://chenghuiz.iteye.com/blog/2310377" target="_blank" rel="external">elasticsearch以及其常用插件安装</a></p>
</li>
<li><p>访问(host:9200/_plugin/plugin<em>name)</em>，例如:</p>
<blockquote>
<p><a href="http://200.200.200.64:9200/_plugin/head/" target="_blank" rel="external">http://200.200.200.64:9200/_plugin/head/</a></p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h4 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h4><ul>
<li><p>将Mysql单表中的数据导入ES</p>
<ul>
<li><p>Mysql单表数据量：1亿行，16GB；导入ES后，占用内存：32GB</p>
<ul>
<li><strong>Note：第一次导入完成后，发现数据丢失，即从Mysql导入一亿行数据后，ES中count(*)仅有7600万行，后续会有原因分析</strong></li>
</ul>
</li>
<li><p>导入方法：</p>
<ul>
<li><p>安装第三方插件：elasticsearch-jdbc，按照git上的操作指引进行数据导入，git地址：<a href="https://github.com/jprante/elasticsearch-jdbc" target="_blank" rel="external">elasticsearch-jdbc</a>，导入脚本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span>!/bin/sh</div><div class="line">export JAVA_HOME=/home/knightyang/jdk1.8.0_131</div><div class="line">export PATH=$&#123;JAVA_HOME&#125;/bin/:$PATH;</div><div class="line">JDBC_IMPORTER_HOME=/home/knightyang/elasticsearch-jdbc-2.3.4.0</div><div class="line">bin=$JDBC_IMPORTER_HOME/bin</div><div class="line">lib=$JDBC_IMPORTER_HOME/lib</div><div class="line">echo '&#123;</div><div class="line">"type" : "jdbc",</div><div class="line">"jdbc": &#123;</div><div class="line">"elasticsearch.autodiscover":true,</div><div class="line">"url":"jdbc:mysql://host:3306/test",  </div><div class="line">"user":"test",  </div><div class="line">"password":"test",  </div><div class="line">"sql":"select *,id  as _id from user_info_yqj ",</div><div class="line">"elasticsearch" : &#123;</div><div class="line">  "host" : "host",</div><div class="line">  "port" : 9300</div><div class="line">&#125;,</div><div class="line">"index" : "yqj_info_index_more",  </div><div class="line">"type" : "yqj_info_type_more"  </div><div class="line">&#125;</div><div class="line">&#125;'| java \</div><div class="line">  -cp "$&#123;lib&#125;/*" \</div><div class="line">  -Dlog4j.configurationFile=$&#123;bin&#125;/log4j2.xml \</div><div class="line">  org.xbib.tools.Runner \</div><div class="line">  org.xbib.tools.JDBCImporter</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>安装bigdesk插件，监控集群资源状态（Memory、CPU、GC、Thread Pool等）</p>
<blockquote>
<p>用于查看执行SQL时的资源消耗</p>
</blockquote>
</li>
<li><p>安装sql插件，可方便的将sql转换成DSL Query</p>
<blockquote>
<p>SQL:  select count(*) from yqj_info_index_more</p>
</blockquote>
</li>
<li><p>安装head插件，可以方便的提交DSL Query</p>
</li>
<li><p>count(*) 统计一亿行数据耗时对比（清理查询缓存后多次执行，取平均值）</p>
<p>| Mysql | ES    |<br>| —– | —– |<br>| 51秒   | 900毫秒 |</p>
<blockquote>
<p>Note: Mysql 执行时间波动较大，从31秒 ~ 80秒，预计于机器负载有关。ES执行时间基本无波动。</p>
</blockquote>
</li>
</ul>
<h4 id="Mysql导入ES丢数据原因分析"><a href="#Mysql导入ES丢数据原因分析" class="headerlink" title="Mysql导入ES丢数据原因分析"></a>Mysql导入ES丢数据原因分析</h4><ul>
<li><p>导入过程：同性能测试中的导入方法</p>
</li>
<li><p>原因分析：</p>
<ul>
<li><p>ES中是否含有重复 _id？</p>
<ul>
<li>没有。ES中采用Mysql表的单一主键值做_id，<strong>不会出现重复</strong></li>
</ul>
</li>
<li><p>Mysql数据中是否有row含有特殊字段？（比如string太长、含有特殊编码字符等）</p>
<ul>
<li>没有。二分查找，找到ES中丢失的一些具体数据，数据本身并没有什么特殊</li>
</ul>
</li>
<li><p>分析Mysql数据导入ES的第三方插件源码: elasticsearch-jdbc</p>
<ul>
<li><p>日志中含有出错信息</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[<span class="number">15</span>:<span class="number">28</span>:<span class="number">15</span>,<span class="number">983</span>][ERROR][org.xbib.elasticsearch.helper.client.BulkTransportClient][elasticsearch[importer][listener][T<span class="comment">#4]] bulk [45] failed with 8020 failed items, failure message = failure in bulk execution:</span></div><div class="line">[<span class="number">0</span>]: index [yqj_info_index_more], type [yqj_info_type_more], id [<span class="number">11598123</span>], message [RemoteTransportException[[Leader][ip:<span class="number">9300</span>][indices:data/write/bulk[s][p]]]; nested: EsRejectedExecutionException[rejected execu</div><div class="line">tion of org.elasticsearch.transport.TransportService$<span class="number">4</span>@<span class="number">7691</span>eb0a on EsThreadPoolExecutor[bulk, queue capacity = <span class="number">50</span>, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@c2f9664[Running, pool size = <span class="number">8</span>, active threads</div><div class="line">= <span class="number">8</span>, queued tasks = <span class="number">50</span>, completed tasks = <span class="number">21008</span>]]];]</div></pre></td></tr></table></figure>
</li>
<li><p>出错原因：导入程序向ES发送数据的速率太快了，超过ES的处理能力。</p>
</li>
<li><p>解决办法：</p>
<ol>
<li>降低导入程序的发送速率 or</li>
<li>增大ES接收数据的线程池数量、缓存队列的size</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="elasticsearch-jdbc-原理分析"><a href="#elasticsearch-jdbc-原理分析" class="headerlink" title="elasticsearch-jdbc 原理分析"></a>elasticsearch-jdbc 原理分析</h4><ul>
<li><p>git地址：<a href="https://github.com/jprante/elasticsearch-jdbc" target="_blank" rel="external">elasticsearch-jdbc</a></p>
</li>
<li><p>处理时序图：</p>
</li>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">title: elasticseach-jdbc时序图</div><div class="line">Runner.main()-&gt; JDBCImport.execute(): 1. 开始向ES导入数据</div><div class="line">Note right of JDBCImport.execute():  启动一个ExecutorServer\n默认线程数为1</div><div class="line">JDBCImport.execute()-&gt; Context.execute(): 2. JDBCImport中构造上下文context\n包含Source(mysql)、Sink(ES)</div><div class="line">Context.execute() -&gt; Context.beforeFetch(): 3. 创建Source、Sink，设置相关参数</div><div class="line">Context.beforeFetch() -&gt; Sink.beforeFetch(): 4. 创建BulkTransportClient，处理fetch前的准备工作\n如：创建ES索引</div><div class="line">Context.beforeFetch() -&gt; Source.beforeFetch(): 5. 这里直接返回</div><div class="line">Context.execute() --&gt; Source.fetch(): 6. 开始fetch数据了</div><div class="line">Source.fetch() -&gt; Source.executeQuery(): 7. 通过JDBC执行SQL</div><div class="line">Source.executeQuery() -&gt; Source.fetch(): 8. 返回SQL结果的迭代器\n</div><div class="line">Source.fetch() -&gt; Source.processRow(): 9. 遍历处理每一行数据\n给数据加上ES的元数据\n如index、type、_id、version等</div><div class="line">Source.processRow() -&gt; Sink.index(): 10. 将数据丢给Sink，构造成IndexRequest，缓存\n当缓存的IndexRequest数量达到一定阈值(10000)\n或经过一定时间后，将这批Request构造成BulkRequest</div><div class="line">Sink.index() -&gt; BulkRequestHandler.execute(): 11. 将BulkRequest经底层Netty发送给ES Server\n若ES处理Failed，log the message</div></pre></td></tr></table></figure>
</li>
<li><p>关键步骤</p>
<ul>
<li><p>JDBCImport中构造上下文context：包含Source(mysql)、Sink(ES)</p>
</li>
<li><p>Sink初始化，创建BulkTransportClient，处理fetch前的准备工作</p>
<ul>
<li><p>如：创建ES索引、设定缓存IndexRequest的阈值，同时处理的BulkRequest最大数量</p>
<blockquote>
<p>Note:  ES处理BulkRequest的线程池默认最大线程数为8，Request缓存队列上线值为50。</p>
<p>若缓存队列满了，还有Request过来就直接抛异常拒绝访问了。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Source中调用JDBC执行SQL，获取SQL结果的迭代器，遍历处理每一行</p>
<ul>
<li>若SQL结果字段名有与ES Control key同名时，就用该字段值替代ES meta。</li>
<li>例如：SQL结果含有字段_id，就将字段值设定为ES存储的文档id</li>
</ul>
</li>
<li><p>Sink将每一行数据封装成IndexRequest，并缓存、处理</p>
<ul>
<li>当缓存的IndexRequest数量达到阈值（10000），将缓存的所有IndexRequest封装成BulkRequest，netty发送给ES Server</li>
<li>当定时线程到期后（默认设定为30秒），就将当前缓存的所有IndexReques封装成BulkRequest，发送给ES Server</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="ES-处理BulkRequest的流程"><a href="#ES-处理BulkRequest的流程" class="headerlink" title="ES 处理BulkRequest的流程"></a>ES 处理BulkRequest的流程</h4><ul>
<li><p>ES启动时，NettyTransport会创建Netty的ServerBootstrap，默认监听端口9300-9400</p>
</li>
<li><p>接收message时，调用ChannelPipeline的ChannelUpStreamHandler依次处理，关键处理类：<code>MessageChannelHandler</code></p>
</li>
<li><p>ES启动时，会注册很多action -&gt; RequestHandler，当收到BulkRequest，调用HandledTransportAction进行处理（action为“data/write/bulk”）</p>
</li>
<li><p>一系列调用后，转到TransportBulkAction.doExecute，根据index分组，若index不存在，会自动创建</p>
</li>
<li><p>TransportBulkAction.executeBulk() 将所有的IndexRequest取出，根据shard分组，构造BulkShardRequest</p>
<ul>
<li>属于同一shard的，都将存储在一起；给IndexRequest分配shard，似乎只是简单轮询</li>
</ul>
</li>
<li><p>遍历BulkShardRequest，调用TransportReplicationAction.doExecute()，Reroute阶段开始（就是将Request发送到Shard所在node）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doExecute</span><span class="params">(Task task, Request request, ActionListener&lt;Response&gt; listener)</span> </span>&#123;</div><div class="line">	<span class="keyword">new</span> ReroutePhase((ReplicationTask) task, request, listener).run();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>从Bulk线程池取出线程处理这个Request，后续存储、建索引过程还未分析</p>
<ul>
<li>默认线程数：8 。Min(处理器核数, 32) </li>
<li>会有个BlockQueue对应这个线程池，若没空闲线程，就将request丢队列，若队列没位置就拒绝这个request</li>
</ul>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>ElasticSearch自身安装简单，但是好些功能（如集群监控、SQL转 DSL Query等）依赖第三方插件<ul>
<li>但是第三方插件稳定性有待商榷。如：Mysql导入ES的插件可能丢失数据（插入失败后未重试）</li>
</ul>
</li>
<li>ElasticSearch能满足实时查询的需求，查询耗时也满足需求，硬件资源消耗也可接受<ul>
<li>一亿级的数据量，count(*)耗时1秒左右</li>
<li>1GB的内存就能顺畅跑起来，执行查询时JVM内存也未见明显波动</li>
</ul>
</li>
</ul>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ul>
<li><a href="http://i.zhcy.tk/blog/elasticsearchyu-solr/" target="_blank" rel="external">搜索引擎选择： Elasticsearch与Solr的调研文档</a></li>
<li><a href="http://www.ctolib.com/topics/79270.html" target="_blank" rel="external">ElasticSearch 5.0 测评以及使用体验</a></li>
<li><a href="http://blog.pengqiuyuan.com/ji-chu-jie-shao-ji-suo-yin-yuan-li-fen-xi/" target="_blank" rel="external">Elasticsearch－基础介绍及索引原理分析</a></li>
<li><a href="http://blog.csdn.net/laoyang360/article/details/52244917" target="_blank" rel="external">Elasticsearch学习，请先看这一篇！</a></li>
<li><a href="http://blog.csdn.net/laoyang360/article/details/52227541" target="_blank" rel="external">Elasticsearch的使用场景深入详解</a></li>
<li><a href="http://www.voidcn.com/blog/bigbigtreewhu/article/p-6323823.html" target="_blank" rel="external">Elasticsearch——Query DSL</a></li>
<li><a href="https://my.oschina.net/jhao104/blog/644909" target="_blank" rel="external">Elasticsearch笔记(一)—Elasticsearch安装配置</a></li>
<li><a href="http://chenghuiz.iteye.com/blog/2310377" target="_blank" rel="external">elasticsearch以及其常用插件安装</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;ElasticSearch调研总结&quot;&gt;&lt;a href=&quot;#ElasticSearch调研总结&quot; class=&quot;headerlink&quot; title=&quot;ElasticSearch调研总结&quot;&gt;&lt;/a&gt;ElasticSearch调研总结&lt;/h3&gt;&lt;h4 id=&quot;Elast
    
    </summary>
    
      <category term="ElasticSearch" scheme="http://yoursite.com/categories/ElasticSearch/"/>
    
    
      <category term="ElasticSearch" scheme="http://yoursite.com/tags/ElasticSearch/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming处理流程源码走读</title>
    <link href="http://yoursite.com/2017/05/03/SparkStreaming%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/"/>
    <id>http://yoursite.com/2017/05/03/SparkStreaming处理流程/</id>
    <published>2017-05-03T00:59:37.651Z</published>
    <updated>2017-05-03T02:17:34.120Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Spark-Streaming处理流程简介"><a href="#Spark-Streaming处理流程简介" class="headerlink" title="Spark Streaming处理流程简介"></a>Spark Streaming处理流程简介</h4><p>​    Spark Streaming 是基于Spark的流式处理框架，会将流式计算分解成一系列短小的批处理作业。Spark Streaming会不停地接收、存储外部数据（如Kafka、MQTT、Socket等），然后每隔一定时间（称之为batch，通常为秒级别的）启动Spark Job来处理这段时间内接收到的数据。</p>
<p>   简单来说，Spark Streaming处理流程为：</p>
<ul>
<li>不停存储外部数据</li>
<li>定期启动Spark Job，处理一个时间段内的数据</li>
</ul>
<h4 id="存储外部数据"><a href="#存储外部数据" class="headerlink" title="存储外部数据"></a>存储外部数据</h4><ul>
<li><p>程序调用流程</p>
<blockquote>
<p>StreamingContext.start()  -&gt;  JobScheduler.start()  -&gt; ReceiverTracker.start()  &amp;&amp; JobGenerator.start()</p>
</blockquote>
<ul>
<li>ReceiverTracker.start() 不停地存储外部数据</li>
<li>JobGenerator.start() 用于处理数据</li>
</ul>
</li>
<li><p>ReceiverTracker</p>
<ul>
<li><p>位于Driver端，用于管理所有的Receiver</p>
<ul>
<li><p>Note：所有ReceiverInputDStream类型的DStream 都对应一个Receiver（用于接收外部数据）</p>
<blockquote>
<p>ReceiverInputDStream.getReceiver() 返回Receiver</p>
</blockquote>
</li>
</ul>
</li>
<li><p>内含ReceivedBlockTracker类型成员</p>
<ul>
<li><p>ReceivedBlockTracker的2个重要方法</p>
<ul>
<li><p>def addBlock(receivedBlockInfo: ReceivedBlockInfo): Boolean</p>
<blockquote>
<p>记录worker发送过来的BlockInfo，存储格式：streamId -&gt; mutable.Queue[ReceivedBlockInfo]</p>
</blockquote>
</li>
<li><p>def allocateBlocksToBatch(batchTime: Time): Unit</p>
<blockquote>
<p>构造Time -&gt; Map[Int, Seq[ReceivedBlockInfo]] ，即构造每个batch对应的流以及Blocks的映射关系</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>构造endpoint，处理ReceiverTrackerLocalMessage类型的本地消息</p>
<ul>
<li>case StartAllReceivers(receivers)</li>
<li>case RestartReceiver(receiver)</li>
<li>case c: CleanupOldBlocks</li>
<li>case UpdateReceiverRateLimit(streamUID, newRate)</li>
<li>case ReportError(streamId, message, error)</li>
</ul>
</li>
<li><p>start()</p>
<ul>
<li><p>启动endpoint，发送StartAllReceivers Message</p>
</li>
<li><p>Note: 在发送StartAllReceivers Message前，执行了runDummySparkJob，用于避免所有receiver被分配到同一个Executor。（设置分区数为50可以确保启动的所有task很小的概率分配到同一host上，而该Spark Job运行结束后，未执行SparkContext.stop，故而BlockManagerMaster中存储的各work的executor信息未清空，可以用于后续需求）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Run the dummy Spark job to ensure that all slaves have registered. This avoids all the</div><div class="line"> * receivers to be scheduled on the same node.</div><div class="line"> *</div><div class="line"> * TODO Should poll the executor number and wait for executors according to</div><div class="line"> * "spark.scheduler.minRegisteredResourcesRatio" and</div><div class="line"> * "spark.scheduler.maxRegisteredResourcesWaitingTime" rather than running a dummy job.</div><div class="line"> */</div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runDummySparkJob</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (!ssc.sparkContext.isLocal) &#123;</div><div class="line">    ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">50</span>, <span class="number">50</span>).map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _, <span class="number">20</span>).collect()</div><div class="line">  &#125;</div><div class="line">  assert(getExecutors.nonEmpty)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>startReceiver()。ReceiverTracker收到自己发送的StartAllReceivers  Message后，对每个receiver执行startReceiver()</p>
<ul>
<li><p>构造RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> receiverRDD: <span class="type">RDD</span>[<span class="type">Receiver</span>[_]] = ssc.sc.makeRDD(<span class="type">Seq</span>(receiver), <span class="number">1</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>指定RDD将执行的function</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> startReceiverFunc: <span class="type">Iterator</span>[<span class="type">Receiver</span>[_]] =&gt; <span class="type">Unit</span> =</div><div class="line">(iterator: <span class="type">Iterator</span>[<span class="type">Receiver</span>[_]]) =&gt; &#123;</div><div class="line">  <span class="keyword">if</span> (!iterator.hasNext) &#123;</div><div class="line">	<span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(</div><div class="line">	  <span class="string">"Could not start receiver as object not found."</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (<span class="type">TaskContext</span>.get().attemptNumber() == <span class="number">0</span>) &#123;</div><div class="line">	<span class="keyword">val</span> receiver = iterator.next()</div><div class="line">	assert(iterator.hasNext == <span class="literal">false</span>)</div><div class="line">	<span class="keyword">val</span> supervisor = <span class="keyword">new</span> <span class="type">ReceiverSupervisorImpl</span>(</div><div class="line">	  receiver, <span class="type">SparkEnv</span>.get, serializableHadoopConf.value, checkpointDirOption)</div><div class="line">	supervisor.start()</div><div class="line">	supervisor.awaitTermination()</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">	<span class="comment">// It's restarted by TaskScheduler, but we want to reschedule it again. So exit it.</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<blockquote>
<p>Note：RDD实际执行ReceiverSupervisorImpl.start()，task失败后重试时将重新调度</p>
</blockquote>
</li>
<li><p>提交Spark Job</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"> <span class="keyword">val</span> future = ssc.sparkContext.submitJob[<span class="type">Receiver</span>[_], <span class="type">Unit</span>, <span class="type">Unit</span>](</div><div class="line">receiverRDD, startReceiverFunc, <span class="type">Seq</span>(<span class="number">0</span>), (_, _) =&gt; <span class="type">Unit</span>, ())</div></pre></td></tr></table></figure>
<blockquote>
<p>至此，Streaming启动了Spark Job，不停的接收外部数据</p>
</blockquote>
</li>
<li><p><strong>ReceiverSupervisorImpl</strong>   直接接收外部数据的关键类，重点分析</p>
<ul>
<li><p><strong>BlockGenerator</strong>。ReceiverSupervisorImpl包含的重要数据成员</p>
<ul>
<li><p>def addData(data: Any): Unit</p>
<blockquote>
<p>将data存入currentBuffer（ArrayBuffer类型）</p>
</blockquote>
</li>
<li><p>updateCurrentBuffer()</p>
<blockquote>
<p>将currentBuffer中的数据构造成Block（使用time做block的uniq id），然后重新构造新的currentBuffer，将Block push 到blocksForPushing 队列（后续有线程不停处理该队列中的block）</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateCurrentBuffer</span></span>(time: <span class="type">Long</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">var</span> newBlock: <span class="type">Block</span> = <span class="literal">null</span></div><div class="line">      synchronized &#123;</div><div class="line">        <span class="keyword">if</span> (currentBuffer.nonEmpty) &#123;</div><div class="line">          <span class="keyword">val</span> newBlockBuffer = currentBuffer</div><div class="line">          currentBuffer = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Any</span>]</div><div class="line">          <span class="keyword">val</span> blockId = <span class="type">StreamBlockId</span>(receiverId, time - blockIntervalMs)</div><div class="line">          listener.onGenerateBlock(blockId)</div><div class="line">          newBlock = <span class="keyword">new</span> <span class="type">Block</span>(blockId, newBlockBuffer)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (newBlock != <span class="literal">null</span>) &#123;</div><div class="line">        blocksForPushing.put(newBlock)  <span class="comment">// put is blocking when queue is full</span></div><div class="line">      &#125;</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
</li>
<li><p>keepPushingBlocks（）</p>
<blockquote>
<p>从blocksForPushing队列中不停地取block，然后处理block。</p>
<p>处理block可分为2个步骤：</p>
<ol>
<li>将block添加进worker本地的BlockManager中</li>
<li>将blockInfo发送到Driver的ReceiverTracker，至此Driver就能感知外部数据了</li>
</ol>
</blockquote>
</li>
<li><p>2个线程之blockIntervalTimer</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> blockIntervalTimer =</div><div class="line">  <span class="keyword">new</span> <span class="type">RecurringTimer</span>(clock, blockIntervalMs, updateCurrentBuffer, <span class="string">"BlockGenerator"</span>)</div></pre></td></tr></table></figure>
<p> 该Timer会启一个线程，周期性的（默认间隔为200ms）调用updateCurrentBuffer</p>
</li>
<li><p>2个线程之blockPushingThread，调用keepPushingBlocks</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> blockPushingThread = <span class="keyword">new</span> <span class="type">Thread</span>() &#123; <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123; keepPushingBlocks() &#125; &#125;</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>ReceiverSupervisorImpl.start()  //回到该方法，继续分析</p>
<ul>
<li><p>onStart()</p>
<blockquote>
<p>BlockGenerator.start()，做好接收外部数据的准备，外部数据会存放在BlockGenerator的currentBuffer中</p>
</blockquote>
</li>
<li><p>onReceiverStart()</p>
<blockquote>
<p>向Driver端的ReceiverTracker注册Receiver</p>
</blockquote>
</li>
<li><p>receiver.onStart()</p>
<blockquote>
<p>即 ReceiverInputDStream.getReceiver().onStart()，用于实际接收外部数据，传递外部数据到BlockGenerator.addData</p>
<p>例如MQTTReceiver.onStart()执行流程: 创建MQTT连接，接收topic，将接收到的message存入BlockGenerator.currentBuffer</p>
</blockquote>
</li>
</ul>
</li>
<li><p>至此，ReceiverTracker 接收外部数据的流程分析完毕，总结为</p>
<blockquote>
<ol>
<li>Driver端的ReceiverTracker 启动Spark Job</li>
<li>worker上调用MQTTReceiver.onStart() 接收外部MQTT数据，并存入BlockGenerator.currentBuffer</li>
<li>BlockGenerator周期性将currentBuffer构造成Block，并同步BlockInfo到Driver的ReceiverTracker</li>
<li>ReceiverTracker能感知Blocks</li>
</ol>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>  ​</p>
<h4 id="定期启动Spark-Job"><a href="#定期启动Spark-Job" class="headerlink" title="定期启动Spark Job"></a>定期启动Spark Job</h4><ul>
<li><p>GenerateJobs</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> timer = <span class="keyword">new</span> <span class="type">RecurringTimer</span>(clock, ssc.graph.batchDuration.milliseconds,</div><div class="line">  longTime =&gt; eventLoop.post(<span class="type">GenerateJobs</span>(<span class="keyword">new</span> <span class="type">Time</span>(longTime))), <span class="string">"JobGenerator"</span>)</div></pre></td></tr></table></figure>
<p>JobGenerator.start() 后，周期性（周期为构造StreamingContext时传入的batch时间参数）调用GenerateJobs。</p>
<p>generateJobs执行步骤：</p>
<ol>
<li><p>ReceiverTracker.allocateBlocksToBatch()</p>
<blockquote>
<p>收集当前batch下，各inputStream产生的BlockInfos</p>
</blockquote>
</li>
<li><p>createBlockRDD(validTime, blockInfos)</p>
<blockquote>
<p>根据上述BlockInfos 创建BlockRDD</p>
</blockquote>
</li>
<li><p>构造jobFunction 与 job</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">getOrCompute(time) <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">Some</span>(rdd) =&gt; &#123;</div><div class="line">    <span class="keyword">val</span> jobFunc = () =&gt; &#123;</div><div class="line">      <span class="keyword">val</span> emptyFunc = &#123; (iterator: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; &#123;&#125; &#125;</div><div class="line">      context.sparkContext.runJob(rdd, emptyFunc)</div><div class="line">    &#125;</div><div class="line">    <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">Job</span>(time, jobFunc))</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="type">None</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p> Job.run() 就是直接调用jobFunc()，运行新的Spark Job，处理rdd</p>
</li>
<li><p>将Job封装成JobSet，丢给线程池（默认一个线程）去实际执行job.run</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">jobSet.jobs.foreach(job =&gt; jobExecutor.execute(<span class="keyword">new</span> <span class="type">JobHandler</span>(job)))</div></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ul>
<ul>
<li><p>总结</p>
<ol>
<li><p>启动独立的Spark Job用于接收外部数据。worker 接收外部数据，周期性(与batch值无关，默认200ms)封装成Block，存入Spark内存中，并将BlockInfo同步给Driver。 </p>
<blockquote>
<p>实际执行Class：BlockGenerator</p>
</blockquote>
</li>
<li><p>Driver周期性（batch值）根据BlockInfos生成BlockRDD，根据RDD构造Spark Job，并执行。</p>
</li>
</ol>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;Spark-Streaming处理流程简介&quot;&gt;&lt;a href=&quot;#Spark-Streaming处理流程简介&quot; class=&quot;headerlink&quot; title=&quot;Spark Streaming处理流程简介&quot;&gt;&lt;/a&gt;Spark Streaming处理流程简介&lt;/
    
    </summary>
    
      <category term="Spark-Streaming" scheme="http://yoursite.com/categories/Spark-Streaming/"/>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="Spark-Streaming" scheme="http://yoursite.com/tags/Spark-Streaming/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2017/05/02/hello-world/"/>
    <id>http://yoursite.com/2017/05/02/hello-world/</id>
    <published>2017-05-02T01:09:27.373Z</published>
    <updated>2017-05-02T01:22:12.520Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
